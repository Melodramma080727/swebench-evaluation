name: SWE-bench Real Agents Framework Evaluation

on:
  workflow_dispatch:
    inputs:
      num_instances:
        description: 'Number of instances to test (1-10 recommended for first test)'
        required: false
        default: '1'
      agent_repo:
        description: 'Agents repository URL'
        required: false
        default: 'https://github.com/Melodramma080727/Agents.git'

jobs:
  # Job 1: Build Agents Docker image and generate patches
  generate_with_agents:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours for real agent execution
    
    steps:
    - name: Maximize build space
      run: |
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /opt/ghc
        sudo rm -rf /usr/local/share/boost
        df -h
        
    - name: Checkout this repository
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Clone Agents framework repository
      run: |
        cd /tmp
        
        echo "Attempting to clone Agents repository..."
        
        # Method 1: Try with GitHub Actions default token
        if git clone https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/Melodramma080727/Agents.git agents 2>/dev/null; then
          echo "âœ… Cloned with GITHUB_TOKEN"
        # Method 2: Try with personal access token if available
        elif [ -n "${{ secrets.GH_PAT }}" ]; then
          echo "Trying with GH_PAT..."
          git clone https://${{ secrets.GH_PAT }}@github.com/Melodramma080727/Agents.git agents
          echo "âœ… Cloned with GH_PAT"
        # Method 3: Try public clone
        else
          echo "Trying public clone..."
          git clone https://github.com/Melodramma080727/Agents.git agents
          echo "âœ… Cloned publicly"
        fi
        
        if [ ! -d agents ]; then
          echo "âŒ Failed to clone Agents repository"
          echo ""
          echo "ðŸ’¡ Solutions:"
          echo "   1. Make the Agents repository public, OR"
          echo "   2. Add GH_PAT secret (Settings â†’ Secrets â†’ New secret)"
          echo "      Name: GH_PAT"
          echo "      Value: Your GitHub Personal Access Token with 'repo' scope"
          echo "      Create at: https://github.com/settings/tokens"
          exit 1
        fi
        
        cd agents
        echo "âœ… Agents repository ready"
        git log --oneline -5
        
    - name: Set up environment for Agents
      run: |
        cd /tmp/agents
        
        # Create .env file with API key (for generate_config.sh)
        cat > .env << EOF
        LLM_PROVIDER=anthropic
        LLM_MODEL=claude-sonnet-4-20250514
        LLM_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}
        EOF
        
        # Create config file matching local working format
        cat > workspace/config.docker.toml << EOF
        # Generated for SWE-bench evaluation
        
        [core]
        runtime = "cli"
        max_iterations = 50
        file_store = "local"
        file_store_path = "/workspace/.openhands"
        workspace_base = "/workspace/projects"
        
        [llm]
        api_key = "${{ secrets.ANTHROPIC_API_KEY }}"
        model = "claude-sonnet-4-20250514"
        temperature = 0.0
        max_tokens = 4096
        
        [sandbox]
        trusted_dirs = ["/workspace", "/app"]
        
        [security]
        confirmation_mode = false
        EOF
        
        # Create settings.json to skip initial setup
        mkdir -p workspace/.openhands
        cat > workspace/.openhands/settings.json << EOF
        {
          "LLM_MODEL": "claude-sonnet-4-20250514",
          "LLM_API_KEY": "${{ secrets.ANTHROPIC_API_KEY }}",
          "LLM_BASE_URL": null,
          "AGENT": "CodeActAgent",
          "CONFIRMATION_MODE": false,
          "ENABLE_DEFAULT_CONDENSER": false
        }
        EOF
        
        echo "âœ… Environment configured"
        echo "Config file:"
        cat workspace/config.docker.toml
        
    - name: Build Agents Docker image
      run: |
        cd /tmp/agents
        echo "Building Docker image..."
        docker compose build agent
        echo "Image built successfully"
        docker images | grep keplore
        
    - name: Set up Python for orchestration
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install Python dependencies
      run: |
        pip install datasets tqdm toml
        
    - name: Create SWE-bench task runner
      run: |
        cat > /tmp/run_agents_real.py << 'PYEOF'
        #!/usr/bin/env python3
        """
        Real Agents Framework Runner for SWE-bench
        Runs each task through the actual Agents Docker container
        """
        
        import json
        import os
        import subprocess
        import sys
        import time
        import toml
        from pathlib import Path
        from datetime import datetime
        from datasets import load_dataset
        
        AGENTS_DIR = Path("/tmp/agents")
        OUTPUT_DIR = Path("/tmp/swebench_agents_output")
        OUTPUT_DIR.mkdir(exist_ok=True)
        
        DATASET_NAME = "princeton-nlp/SWE-bench_Lite"
        MODEL_NAME = "claude-sonnet-4-20250514"
        NUM_INSTANCES = int(os.getenv("NUM_INSTANCES", "1"))
        
        def create_agent_task(instance):
            """Create task prompt for the Agents framework."""
            return f"""You are working on the {instance['repo']} repository.
        
        TASK: Fix the following GitHub issue by generating a complete git diff patch.
        
        Issue ID: {instance['instance_id']}
        Base Commit: {instance.get('base_commit', 'N/A')}
        
        Issue Description:
        {instance['problem_statement']}
        
        INSTRUCTIONS:
        1. Analyze the issue carefully
        2. Identify the files that need to be modified
        3. Make the necessary code changes
        4. Generate a complete git diff patch showing all changes
        
            TASK_INSTRUCTIONS: |
              Task: Fix the Flask Blueprint validation issue
              
              IMPORTANT: The correct code structure in src/flask/blueprints.py around line 191:
              ```python
              # Line 188-195 context:
                      template_folder=template_folder,
                      root_path=root_path,
                  )
                  self.name = name  # This is line 191
                  self.url_prefix = url_prefix
                  self.subdomain = subdomain
              ```
              
              You need to add validation AFTER line 191 where `self.name = name` is set.
              
              SPECIFIC REQUIREMENTS:
              1. Add assertion: `assert "." not in name, "Blueprint names should not contain dots"`
              2. Insert this AFTER `self.name = name` (line 191)
              3. Use proper indentation (8 spaces)
              
              OUTPUT REQUIREMENT:
              At the end of your work, you MUST create a file called 'solution.patch' in the workspace containing ONLY the git diff patch in standard unified diff format.
              
              IMPORTANT: Use a command like:
              cat > solution.patch << 'EOF'
              diff --git a/src/flask/blueprints.py b/src/flask/blueprints.py
              index [hash]..[hash] 100644
              --- a/src/flask/blueprints.py
              +++ b/src/flask/blueprints.py
              @@ -188,6 +188,7 @@ class Blueprint(Scaffold):
                       template_folder=template_folder,
                       root_path=root_path,
                   )
              +        assert "." not in name, "Blueprint names should not contain dots"
                   self.name = name
                   self.url_prefix = url_prefix
                   self.subdomain = subdomain
              EOF
              
              Then verify the file exists: ls -la solution.patch
              
              CRITICAL: Use the EXACT line numbers and context shown above.
        The patch must be complete and ready to apply with 'git apply'.
        """
        
        def run_agent_on_task(instance, instance_idx, total):
            """Run the Agents framework on a single SWE-bench task."""
            instance_id = instance['instance_id']
            
            print(f"\n{'='*80}")
            print(f"[{instance_idx}/{total}] Processing: {instance_id}")
            print(f"Repository: {instance['repo']}")
            print(f"{'='*80}")
            
            # Create task
            task = create_agent_task(instance)
            
            # Create workspace for this instance
            instance_workspace = AGENTS_DIR / "workspace" / "projects" / instance_id
            instance_workspace.mkdir(parents=True, exist_ok=True)
            
            # Save task description
            task_file = instance_workspace / "task.txt"
            task_file.write_text(task)
            
            print(f"Task created ({len(task)} chars)")
            print(f"Workspace: {instance_workspace}")
            
            # Run agent using docker compose
            start_time = time.time()
            
            try:
                # Build command - disable TTY for non-interactive mode
                # Pass ANTHROPIC_API_KEY as environment variable
                cmd = [
                    "docker", "compose", "run", "--rm",
                    "-T",  # Disable pseudo-TTY allocation (non-interactive)
                    "-e", f"ANTHROPIC_API_KEY={os.getenv('ANTHROPIC_API_KEY')}",
                    "-e", "OPENHANDS_SKIP_CONFIRMATIONS=true",  # Skip all confirmations
                    "agent",  # No volume override - use docker-compose.yml default mount
                    "--task", task,
                    "--max-iterations", "500"  # Very high limit to avoid iteration errors during SWE-bench testing
                ]
                
                print(f"Running Agent (timeout: 10 minutes for quick test)...")
                print(f"Command: {' '.join(cmd)}")
                print("=" * 80)
                sys.stdout.flush()
                
                # Use Popen for real-time output
                # Keep stdin open to send commands when agent waits for input
                process = subprocess.Popen(
                    cmd,
                    cwd=AGENTS_DIR,
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,  # Line buffered
                    universal_newlines=True
                )
                
                # Stream output in real-time and detect if agent is waiting
                output_lines = []
                import threading
                import queue
                
                output_queue = queue.Queue()
                
                def read_output():
                    for line in process.stdout:
                        output_queue.put(line)
                    output_queue.put(None)  # Signal end
                
                reader_thread = threading.Thread(target=read_output)
                reader_thread.daemon = True
                reader_thread.start()
                
                # Monitor output and auto-respond to prompts
                while True:
                    try:
                        line = output_queue.get(timeout=1)
                        if line is None:
                            break
                        print(line, end='', flush=True)
                        output_lines.append(line)
                        
                        # Detect if agent is waiting for input
                        if "Agent is waiting for your input" in line or "Before I continue" in line:
                            time.sleep(1)
                            try:
                                # Re-send the complete task description when agent is waiting
                                process.stdin.write(f"{task}\n")
                                process.stdin.flush()
                            except:
                                pass
                        
                        # Detect task completion and send exit
                        # Multiple patterns for different completion states
                        if ("Task completed" in line or 
                            "Stop acknowledged" in line or 
                            "Share another instruction" in line or
                            "let me know when you're ready to resume" in line or
                            "Closed conversation" in line):
                            print("\nðŸŽ¯ Task completion detected! Sending exit command...")
                            time.sleep(2)  # Wait for any final output
                            try:
                                process.stdin.write("exit\n")
                                process.stdin.flush()
                                process.stdin.close()
                            except:
                                pass
                            
                            # Wait a bit for graceful exit, then force terminate if needed
                            time.sleep(5)
                            if process.poll() is None:
                                print("ðŸ”¥ Forcing process termination...")
                                process.terminate()
                                time.sleep(2)
                                if process.poll() is None:
                                    process.kill()
                            break  # Exit the monitoring loop
                    except queue.Empty:
                        if process.poll() is not None:
                            break
                
                # Wait for completion (30 minutes for complex task with high iteration limit)
                return_code = process.wait(timeout=1800)
                
                # Create result object
                class Result:
                    def __init__(self, returncode, stdout):
                        self.returncode = returncode
                        self.stdout = stdout
                        self.stderr = ""
                
                result = Result(return_code, ''.join(output_lines))
                
                elapsed = time.time() - start_time
                print(f"Agent completed in {elapsed:.1f}s")
                print(f"Return code: {result.returncode}")
                
                # Show agent output for debugging
                if result.stdout:
                    print("=== Agent STDOUT (first 2000 chars) ===")
                    print(result.stdout[:2000])
                if result.stderr:
                    print("=== Agent STDERR (first 2000 chars) ===")
                    print(result.stderr[:2000])
                
                # Extract patch
                patch = extract_patch_from_workspace(instance_workspace, result.stdout)
                
                if not patch:
                    print(f"âš ï¸  No patch found")
                    # Save output for debugging
                    debug_file = OUTPUT_DIR / f"{instance_id}_output.log"
                    debug_file.write_text(f"STDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}")
                    
                    return {
                        "instance_id": instance_id,
                        "model_name_or_path": "real_agents_framework",
                        "model_patch": "",
                        "error": "No patch generated",
                        "elapsed_time": elapsed
                    }
                
                print(f"âœ… Patch extracted ({len(patch)} chars, {len(patch.split(chr(10)))} lines)")
                
                # Save patch
                patch_file = OUTPUT_DIR / f"{instance_id}.patch"
                patch_file.write_text(patch)
                
                return {
                    "instance_id": instance_id,
                    "model_name_or_path": "real_agents_framework",
                    "model_patch": patch,
                    "elapsed_time": elapsed
                }
                
            except subprocess.TimeoutExpired:
                elapsed = time.time() - start_time
                print(f"âŒ Timeout after {elapsed:.1f}s")
                return {
                    "instance_id": instance_id,
                    "model_name_or_path": "real_agents_framework",
                    "model_patch": "",
                    "error": "Timeout",
                    "elapsed_time": elapsed
                }
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âŒ Error: {e}")
                import traceback
                traceback.print_exc()
                return {
                    "instance_id": instance_id,
                    "model_name_or_path": "real_agents_framework",
                    "model_patch": "",
                    "error": str(e),
                    "elapsed_time": elapsed
                }
        
        def extract_patch_from_workspace(workspace_dir, agent_output):
            """Extract patch from agent workspace or output."""
            # Method 1: Look for solution.patch file
            solution_file = workspace_dir / "solution.patch"
            if solution_file.exists():
                content = solution_file.read_text()
                if content.strip().startswith('diff --git'):
                    return content.strip()
            
            # Method 2: Look for any .patch or .diff files
            for ext in ['.patch', '.diff']:
                for patch_file in workspace_dir.rglob(f'*{ext}'):
                    content = patch_file.read_text()
                    if content.strip().startswith('diff --git'):
                        return content.strip()
            
            # Method 3: Extract from agent output
            if 'diff --git' in agent_output:
                lines = agent_output.split('\n')
                patch_lines = []
                in_patch = False
                
                for line in lines:
                    if line.startswith('diff --git'):
                        in_patch = True
                    if in_patch:
                        patch_lines.append(line)
                        # Simple heuristic: stop at clear end markers
                        if line.strip() in ['```', '---END---', 'Task completed']:
                            break
                
                if patch_lines:
                    return '\n'.join(patch_lines).strip()
            
            return ""
        
        def main():
            print("="*80)
            print("ðŸ¤– Real Agents Framework Evaluation for SWE-bench")
            print("="*80)
            
            # Load dataset
            print(f"\nLoading {DATASET_NAME}...")
            dataset = load_dataset(DATASET_NAME, split="test")
            
            # Use simple test instance for quick testing
            TEST_INSTANCE = "pallets__flask-4045"  # Simple 230-char issue
            instances = [item for item in dataset if item['instance_id'] == TEST_INSTANCE]
            
            if not instances:
                print(f"âš ï¸  Test instance {TEST_INSTANCE} not found, using first {NUM_INSTANCES}")
                instances = list(dataset.select(range(NUM_INSTANCES)))
            
            print(f"âœ… Testing with {len(instances)} instance(s): {[i['instance_id'] for i in instances]}")
            
            # Output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            predictions_file = OUTPUT_DIR / f"real_agents_predictions_{timestamp}.jsonl"
            
            print(f"Predictions will be saved to: {predictions_file}\n")
            
            # Process instances
            results = []
            successful = 0
            failed = 0
            total_time = 0
            
            with open(predictions_file, 'w') as f:
                for idx, instance in enumerate(instances, 1):
                    result = run_agent_on_task(instance, idx, len(instances))
                    
                    f.write(json.dumps(result) + '\n')
                    f.flush()
                    
                    results.append(result)
                    total_time += result.get('elapsed_time', 0)
                    
                    if result.get('model_patch'):
                        successful += 1
                    else:
                        failed += 1
            
            # Summary
            print("\n" + "="*80)
            print("SUMMARY")
            print("="*80)
            print(f"Total instances: {len(instances)}")
            print(f"Successful: {successful}")
            print(f"Failed: {failed}")
            print(f"Success rate: {successful/len(instances)*100:.1f}%")
            print(f"Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
            print(f"Avg time per instance: {total_time/len(instances):.1f}s")
            print(f"\nPredictions: {predictions_file}")
            print("="*80)
            
            # Output path for next step
            print(predictions_file)
        
        if __name__ == "__main__":
            main()
        PYEOF
        
        chmod +x /tmp/run_agents_real.py
        
    - name: Verify API key is set
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        if [ -z "$ANTHROPIC_API_KEY" ]; then
          echo "âŒ ERROR: ANTHROPIC_API_KEY secret is not set!"
          echo ""
          echo "ðŸ“ To fix this:"
          echo "1. Go to: https://github.com/Melodramma080727/swebench-evaluation/settings/secrets/actions"
          echo "2. Click 'New repository secret'"
          echo "3. Name: ANTHROPIC_API_KEY"
          echo "4. Value: Your Anthropic API key (sk-ant-...)"
          echo "5. Click 'Add secret'"
          exit 1
        else
          echo "âœ… ANTHROPIC_API_KEY is set (length: ${#ANTHROPIC_API_KEY})"
        fi
        
    - name: Run Agents Framework on SWE-bench tasks
      env:
        NUM_INSTANCES: ${{ github.event.inputs.num_instances || '1' }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        echo "API Key length: ${#ANTHROPIC_API_KEY}"
        export ANTHROPIC_API_KEY="${ANTHROPIC_API_KEY}"
        python3 /tmp/run_agents_real.py 2>&1 | tee /tmp/agents_run.log
        
    - name: Upload predictions artifact
      uses: actions/upload-artifact@v4
      with:
        name: real-agents-predictions
        path: /tmp/swebench_agents_output/*.jsonl
        retention-days: 7
        
    - name: Upload patches artifact
      uses: actions/upload-artifact@v4
      with:
        name: real-agents-patches
        path: /tmp/swebench_agents_output/*.patch
        retention-days: 7
        
    - name: Upload debug logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: real-agents-logs
        path: |
          /tmp/agents_run.log
          /tmp/swebench_agents_output/*.log
        retention-days: 7

  # Job 2: Evaluate the patches
  evaluate_patches:
    needs: generate_with_agents
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y git build-essential
        
    - name: Clone and install SWE-bench
      run: |
        cd /tmp
        git clone https://github.com/SWE-bench/SWE-bench.git
        cd SWE-bench
        pip install -e .
        pip install --upgrade docker==7.1.0
        
    - name: Download predictions
      uses: actions/download-artifact@v4
      with:
        name: real-agents-predictions
        path: /tmp/predictions
        
    - name: List downloaded files
      run: |
        echo "Downloaded predictions:"
        ls -lh /tmp/predictions/
        echo "Content preview:"
        head -100 /tmp/predictions/*.jsonl || echo "No predictions found"
        
    - name: Run SWE-bench evaluation
      run: |
        cd /tmp/SWE-bench
        
        PRED_FILE=$(ls /tmp/predictions/*.jsonl | head -1)
        
        if [ -z "$PRED_FILE" ]; then
          echo "âŒ No predictions file found!"
          exit 1
        fi
        
        echo "Evaluating: $PRED_FILE"
        
        # Run evaluation
        python3 -m swebench.harness.run_evaluation \
            --dataset_name princeton-nlp/SWE-bench_Lite \
            --predictions_path "$PRED_FILE" \
            --max_workers 1 \
            --run_id real_agents_$(date +%Y%m%d_%H%M%S)
        
    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      with:
        name: real-agents-evaluation-results
        path: |
          /tmp/SWE-bench/*.json
          /tmp/SWE-bench/logs/
        retention-days: 30
        
    - name: Display results
      run: |
        echo "=== Evaluation Results ==="
        cat /tmp/SWE-bench/*.json 2>/dev/null || echo "No results file found"
